{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd396687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bahar/anaconda3/envs/thesis/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#Importing libraries\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "import keras.utils as ku \n",
    "\n",
    "from tensorflow import keras\n",
    "import keras_nlp\n",
    "\n",
    "# set seeds for reproducability\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(2)\n",
    "seed(1)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23ed3b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "821"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "\n",
    "curr_dir = 'archive/'\n",
    "all_headlines = []\n",
    "for filename in os.listdir(curr_dir):\n",
    "    if 'Articles' in filename:\n",
    "        article_df = pd.read_csv(curr_dir + filename)\n",
    "        all_headlines.extend(list(article_df.headline.values))\n",
    "        break\n",
    "\n",
    "all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
    "len(all_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "288c8c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rhythm of the streets were warrior women and yes we can play',\n",
       " 'as deficit grows congress keeps spending',\n",
       " 'lesson in select bus service',\n",
       " 'heres the real state of the union',\n",
       " 'good riddance to chief wahoo',\n",
       " 'in south africa facing day zero with no water',\n",
       " 'how trumps critics should respond',\n",
       " 'a republican stalwart sets out on a quest to unseat cuomo as governor',\n",
       " 'beirut trailer was supposed to thrill it didnt',\n",
       " 'worker who sent missile alert had confused drills for the real thing before']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset preprocessing\n",
    "\n",
    "def clean_text(txt):\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return txt \n",
    "\n",
    "corpus = [clean_text(x) for x in all_headlines]\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "202d183f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[753, 4],\n",
       " [753, 4, 1],\n",
       " [753, 4, 1, 228],\n",
       " [753, 4, 1, 228, 161],\n",
       " [753, 4, 1, 228, 161, 754],\n",
       " [753, 4, 1, 228, 161, 754, 84],\n",
       " [753, 4, 1, 228, 161, 754, 84, 7],\n",
       " [753, 4, 1, 228, 161, 754, 84, 7, 162],\n",
       " [753, 4, 1, 228, 161, 754, 84, 7, 162, 38],\n",
       " [753, 4, 1, 228, 161, 754, 84, 7, 162, 38, 48]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N-gram Tokenization\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words\n",
    "\n",
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
    "inp_sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cfe82c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sequence padding\n",
    "\n",
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97d00a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 16)]              0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 16, 50)           126450    \n",
      " g (TokenAndPositionEmbeddin                                     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " transformer_decoder (Transf  (None, 16, 50)           15094     \n",
      " ormerDecoder)                                                   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 512)               866304    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2512)              1288656   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,296,504\n",
      "Trainable params: 2,296,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model using Transformer Decoder & GRU Neural Network\n",
    "\n",
    "embed_dim = 50\n",
    "num_heads = 4\n",
    "rnn_units = 512\n",
    "maxlen = max_sequence_len\n",
    "vocab_size = total_words\n",
    "\n",
    "def create_model():\n",
    "    inputs = keras.layers.Input(shape=(maxlen-1,), dtype=tf.int32)\n",
    "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(vocab_size, maxlen, embed_dim)(inputs)\n",
    "    decoder = keras_nlp.layers.TransformerDecoder(intermediate_dim=embed_dim, \n",
    "                                                            num_heads=num_heads, \n",
    "                                                            dropout=0.5)(embedding_layer)\n",
    "    gru_layer = GRU(rnn_units, return_sequences=False)(decoder)\n",
    "\n",
    "    flat_layer = Flatten()(gru_layer)\n",
    "    \n",
    "    outputs = keras.layers.Dense(vocab_size, activation='softmax')(flat_layer)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=\"adam\", \n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d45abe8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "20/20 [==============================] - 5s 146ms/step - loss: 7.4741 - accuracy: 0.0244\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 3s 146ms/step - loss: 7.0047 - accuracy: 0.0282\n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 3s 146ms/step - loss: 6.8917 - accuracy: 0.0351\n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 3s 132ms/step - loss: 6.8195 - accuracy: 0.0369\n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 3s 143ms/step - loss: 6.6621 - accuracy: 0.0496\n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 3s 145ms/step - loss: 6.3737 - accuracy: 0.0695\n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 3s 158ms/step - loss: 5.9752 - accuracy: 0.0937\n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 3s 172ms/step - loss: 5.4775 - accuracy: 0.1145\n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 4s 203ms/step - loss: 4.9195 - accuracy: 0.1631\n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 5s 240ms/step - loss: 4.3241 - accuracy: 0.2229\n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 5s 259ms/step - loss: 3.7709 - accuracy: 0.3064\n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 5s 242ms/step - loss: 3.2132 - accuracy: 0.4072\n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 5s 239ms/step - loss: 2.6861 - accuracy: 0.5041\n",
      "Epoch 14/20\n",
      "20/20 [==============================] - 5s 228ms/step - loss: 2.2225 - accuracy: 0.5969\n",
      "Epoch 15/20\n",
      "20/20 [==============================] - 4s 209ms/step - loss: 1.8439 - accuracy: 0.6579\n",
      "Epoch 16/20\n",
      "20/20 [==============================] - 5s 246ms/step - loss: 1.5205 - accuracy: 0.7210\n",
      "Epoch 17/20\n",
      "20/20 [==============================] - 5s 236ms/step - loss: 1.2604 - accuracy: 0.7728\n",
      "Epoch 18/20\n",
      "20/20 [==============================] - 5s 247ms/step - loss: 1.0527 - accuracy: 0.8101\n",
      "Epoch 19/20\n",
      "20/20 [==============================] - 4s 209ms/step - loss: 0.8826 - accuracy: 0.8422\n",
      "Epoch 20/20\n",
      "20/20 [==============================] - 4s 220ms/step - loss: 0.7641 - accuracy: 0.8611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8014da9ac0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Model\n",
    "\n",
    "model.fit(predictors, label, epochs=20, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2a601e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the text\n",
    "\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        \n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted = np.argmax(predicted,axis=1)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        \n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e198d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "War News And Living In Limbo As\n",
      "Great Mistake Fear Of Shallow Living\n",
      "Donald Trump Is Weak So Is\n",
      "Germany And Austria The Minds So Not\n",
      "President News And Living In Limbo As\n",
      "The Truth Life Is Found To Be Of\n",
      "Science And Technology Race And Politics In San\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"war news\", 5, model, max_sequence_len))\n",
    "print (generate_text(\"great mistake\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"donald trump\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"germany and austria\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"president news\", 5, model, max_sequence_len))\n",
    "print (generate_text(\"the truth life is\", 4, model, max_sequence_len))\n",
    "print (generate_text(\"science and technology\", 5, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f61a9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70a3869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9557a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1440e95d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
